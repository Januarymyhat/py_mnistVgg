# import tensorflow as tf
# from keras import layers, losses, Model
# import os
# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
#
# # os.environ['CUDA_VISIBLE_DEVICES'] = '/gpu:0'
#
# # from tensorflow.python.client import device_lib
# # print(device_lib.list_local_devices())
#
# model = tf.keras.applications.VGG16(weights='imagenet', include_top=False, input_shape=(32, 32, 3))
# for layer in model.layers:
#     layer.trainable = False
#
# x = layers.Flatten()(model.output)
# x = layers.Dense(4096, activation='relu')(x)
# x = layers.Dropout(0.5)(x)
# x = layers.Dense(4096, activation='relu')(x)
# x = layers.Dropout(0.5)(x)
# predictions = layers.Dense(10, activation='softmax')(x)
# model = Model(inputs=model.input, outputs=predictions)
#
# model.compile(optimizer='adam',
#               loss=losses.sparse_categorical_crossentropy,
#               metrics=['accuracy'])
#
#     # Get the data.
# (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data(path='mnist.npz')
#
# x_train = tf.pad(x_train, [[0, 0], [2, 2], [2, 2]])/255
# x_test = tf.pad(x_test, [[0, 0], [2, 2], [2, 2]])/255
# x_train = tf.stack((x_train, x_train, x_train), axis=-1)
# x_test = tf.stack((x_test, x_test, x_test), axis=-1)
#
#
# history = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=10)
#     # save the model
# model.save(r'test/model.h5')